{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Gathering Live Tweets\n",
    "This notebook is the beginning of Part 2 which attempts to validate the findings in Part 1 on real-time conditions with data pulled from the Twitter API and the Yahoo Finance API. In the cell below, I import the necessary tools, connect to the Twitter API, write the function to calculate the sentiment scores of each tweet and define the stocks that I am performing the analysis on.  In this case, I am using the same stocks as in the previous notebooks with the exception of those companies who have been absorbed by other companies and thus no longer have a valid ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import datetime, time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy import stats\n",
    "pd.core.common.is_list_like=pd.api.types.is_list_like\n",
    "import pandas_datareader as pdr\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "vader=SentimentIntensityAnalyzer()\n",
    "pd.core.common.is_list_like=pd.api.types.is_list_like\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "twitter_api_key='TdSeHGzbz3V1MWGUUQaKSY1w2'\n",
    "twitter_api_secret='nulKL65bYeUOmvkt6XuGsChCgKrWh9C0zJmJYPgeqlfcdWgvN2'\n",
    "app_token='983746297135067137-uouYUhYF7uK56qiNu2sQ48PPLtwetvn'\n",
    "app_secret='OxCAz0jgNwngT8kceI1PgbUYqmQgr0c8hRuwQ6dXhJ1p0'\n",
    "auth=tw.OAuthHandler(twitter_api_key,twitter_api_secret)\n",
    "auth.set_access_token(app_token,app_secret)\n",
    "api=tw.API(auth,wait_on_rate_limit=True)\n",
    "\n",
    "def add_sentiment_scores(df,ticker_sym):\n",
    "    vader_score=[]\n",
    "    weighted_vader=[]\n",
    "    blob_score=[]\n",
    "    weighted_blob=[]\n",
    "    ticker=[]\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            text=df['text'][i]\n",
    "            follow=df.followers[i]\n",
    "            vad=vader.polarity_scores(text)\n",
    "            blb=TextBlob(text)\n",
    "            vs=vad['compound']\n",
    "            bl=blb.sentiment[0]\n",
    "            vader_score.append(vs)\n",
    "            weighted_vader.append(vs*follow)\n",
    "            blob_score.append(bl)\n",
    "            weighted_blob.append(bl*follow)\n",
    "            ticker.append(ticker_sym)\n",
    "        except:\n",
    "            vader_score.append(0)\n",
    "            weighted_vader.append(0)\n",
    "            blob_score.append(0)\n",
    "            weighted_blob.append(0)\n",
    "    vader_score=pd.DataFrame(vader_score,columns=['vader_score'])\n",
    "    weighted_vader=pd.DataFrame(weighted_vader,columns=['weighted_vader'])\n",
    "    blob_score=pd.DataFrame(blob_score,columns=['blob_score'])\n",
    "    weighted_blob=pd.DataFrame(weighted_blob,columns=['weighted_blob'])\n",
    "    ticker=pd.DataFrame(ticker,columns=['ticker'])\n",
    "    df=df.merge(vader_score,left_index=True,right_index=True)\n",
    "    df=df.merge(weighted_vader,left_index=True,right_index=True)\n",
    "    df=df.merge(blob_score,left_index=True,right_index=True)\n",
    "    df=df.merge(weighted_blob,left_index=True,right_index=True)\n",
    "    df=df.merge(ticker,left_index=True,right_index=True)\n",
    "    return df\n",
    "\n",
    "STOCKS=['aal','aapl','adbe','adp','adsk','akam','alxn','amat','amgn','amzn','atvi','bbby','bidu','bmrn',\n",
    "        'celg','cern','chkp','chtr','cmcsa','cost','csco','csx','ctrp','dish','dltr','ea','ebay','endp',\n",
    "        'expe','fast','fb','fisv','gild','goog','hsic','ilmn','incy','intu','isrg','jd','khc',\n",
    "        'lrcx','mar','mat','mdlz','mnst','msft','mu','mxim','myl','nclh','nflx','ntap','nvda','nxpi',\n",
    "        'orly','payx','pcar','pypl','qcom','regn','rost','sbac','sbux','srcl','stx','swks','symc',\n",
    "        'tmus','trip','tsco','tsla','txn','ulta','vod','vrsk','vrtx','wba','wdc','xlnx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several things to note:\n",
    "- Depending on what Twitter API package you either pay or do not pay for, there are limitations on how much data and how often you can pull data.\n",
    "- As far as I am able to tell, the free version only allows for tweets within the past week to be gathered.\n",
    "- The commented line in the cell below should only be un-commented on the very first time you run the cell.  After that it will erase any data you have gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kylej\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "#all_api_tweets=pd.DataFrame([])\n",
    "start_date=\"2019-07-31\"\n",
    "for stock in STOCKS:\n",
    "    tweet_holder=[]\n",
    "    for tweet in tw.Cursor(api.search,q=\"$\"+stock,count=1000000,lang=\"en\",since=start_date).items():\n",
    "        tweet_holder.append(tweet)\n",
    "    tweet_df=pd.DataFrame([])\n",
    "    for i in range(len(tweet_holder)):\n",
    "        tweet_df=tweet_df.append(pd.DataFrame({'date_time':tweet_holder[i].created_at,\n",
    "                                               'text':tweet_holder[i].text,\n",
    "                                               'username':tweet_holder[i].user.screen_name,\n",
    "                                               'followers':tweet_holder[i].user.followers_count},index=[0]),ignore_index=True)\n",
    "    tweet_df.date_time=tweet_df.date_time-np.timedelta64(4,\"h\")\n",
    "    tweet_df=add_sentiment_scores(tweet_df,stock)\n",
    "    all_api_tweets=all_api_tweets.append(tweet_df)\n",
    "all_api_tweets.reset_index(drop=True,inplace=True)\n",
    "all_api_tweets['Date']=all_api_tweets.date_time.dt.strftime(\"%Y-%m-%d\")\n",
    "all_api_tweets['Date']=pd.to_datetime(all_api_tweets['Date'], infer_datetime_format=True)\n",
    "pickle_out=open(\"all_api_tweets.pickle\",\"wb\")\n",
    "pickle.dump(all_api_tweets,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Stock Movement Data\n",
    "Here I first identify all of the stocks in questiona and all of the dates in which there are tweets and then find the each stock's movement on each day.  I then create a dataframe that has a row for each stock for each day as well as its various sentiment scores, the number of tweets and the resulting stock movement.  Lastly I calculate each stock's average tweet volume and the standard deviation of that tweet volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathers daily return for each stock on each day of a tweet\n",
    "all_stocks=all_api_tweets.ticker.unique()\n",
    "all_dates=all_api_tweets.Date.unique()\n",
    "all_dates=all_dates[:-1]\n",
    "daily_api_returns=pd.DataFrame(all_dates,columns=['Date'])\n",
    "count=0\n",
    "for i in all_stocks:\n",
    "    holder = pdr.get_data_yahoo(i,'2019-07-22','2019-08-08')\n",
    "    holder['Date'] = holder.index\n",
    "    holder['Date'] = pd.to_datetime(holder['Date'], infer_datetime_format=True)\n",
    "    holder = holder.reset_index(drop=True)\n",
    "    holder['move']=(holder.Close-holder.Open)/holder.Open\n",
    "    holder.drop(['High','Low','Open','Close','Volume','Adj Close'],axis=1,inplace=True)\n",
    "    holder.rename(columns={'move':str(i)},inplace=True)\n",
    "    daily_api_returns=daily_api_returns.merge(holder,left_on='Date',right_on='Date')\n",
    "    count+=1\n",
    "daily_api_returns=daily_api_returns.set_index('Date')\n",
    "all_stocks=all_api_tweets.ticker.unique()\n",
    "df_api=pd.DataFrame([])\n",
    "for i in all_stocks:\n",
    "    # isolating tweets about stock in question\n",
    "    stock_tweets=all_api_tweets[all_api_tweets.ticker==i]\n",
    "    stock_tweets.reset_index(inplace=True)\n",
    "    for j in daily_api_returns.index:\n",
    "        try:\n",
    "            stock_move=daily_api_returns[i][j]\n",
    "            end_time=j+np.timedelta64(9,\"h\")+np.timedelta64(30,'m')\n",
    "            start_time=j-np.timedelta64(1,\"D\")+np.timedelta64(16,'h')\n",
    "            stock_tweets_day=stock_tweets[(stock_tweets.date_time>start_time)&(stock_tweets.date_time<end_time)]\n",
    "            df_api=df_api.append(pd.DataFrame({'stock':i,'trade_date':j,'stock_vader':stock_tweets_day.vader_score.mean(),\n",
    "                                               'stock_weighted_vader':stock_tweets_day.weighted_vader.sum()/stock_tweets_day.followers.sum(),\n",
    "                                               'stock_blob':stock_tweets_day.blob_score.mean(),\n",
    "                                               'stock_weighted_blob':stock_tweets_day.weighted_blob.sum()/stock_tweets_day.followers.sum(),\n",
    "                                               'num_stock_tweets':len(stock_tweets_day),'stock_tweets_reach':stock_tweets_day.followers.sum(),\n",
    "                                               'stock_move':stock_move},index=[0]),ignore_index=True)\n",
    "        except:\n",
    "            print('fail',i,j)\n",
    "df_api=df_api.dropna()\n",
    "df_api.reset_index(drop=True,inplace=True)\n",
    "df_api['stock_num_tweet_mean']=None\n",
    "df_api['stock_num_tweet_std']=None\n",
    "df_api['stock_tweet_sentiment_mean']=None\n",
    "df_api['stock_tweet_sentiment_std']=None\n",
    "sto=df_api.stock.unique()\n",
    "for i in sto:\n",
    "    temp=df_api[df_api.stock==i]\n",
    "    num_mean=temp.num_stock_tweets.mean()\n",
    "    num_std=temp.num_stock_tweets.std()\n",
    "    sent_mean=temp.stock_weighted_vader.mean()\n",
    "    sent_std=temp.stock_weighted_vader.std()\n",
    "    df_api.loc[df_api['stock'].str.contains(i),'stock_num_tweet_mean']=num_mean\n",
    "    df_api.loc[df_api['stock'].str.contains(i),'stock_num_tweet_std']=num_std\n",
    "    df_api.loc[df_api['stock'].str.contains(i),'stock_tweet_sentiment_mean']=sent_mean\n",
    "    df_api.loc[df_api['stock'].str.contains(i),'stock_tweet_sentiment_std']=sent_std\n",
    "pickle_out=open(\"df_api.pickle\",\"wb\")\n",
    "pickle.dump(df_api,pickle_out)\n",
    "pickle_out.close()\n",
    "pickle_out=open(\"daily_api_returns.pickle\",\"wb\")\n",
    "pickle.dump(daily_api_returns,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please continue to the notebook titled \"Notebook_5_Live_Tweet_Analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
