{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Data Gathering of Archived Tweets\n",
    "This notebook contains the data wrangling for Part 1 of this project.  The process in this is admittedly very cumbersome, ugly and time consuming; but ultimately it was a proof of concept before making the process more robust in Part 2.  The raw data was obtained from: http://followthehashtag.com/datasets/nasdaq-100-companies-free-twitter-dataset/ \n",
    "\n",
    "From this website, I downloaded a folder of Excel files for most of the stocks in the NASDAQ 100 (some of the links were broken or duplicates).  Within each folder was an Excel file that contained all tweets mentioning the particular stock between March 2016 and June 2016 and data that went along with the tweet such as the account handle, number of followers and many other things.  I then isolated the raw tweets and then saved them as individual .csv files in a folder in this repository with each filename being the ticker of the stock in question.  Below I begin by importing the necessary libraries and installing both TextBlob and Vader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "import os\n",
    "pd.core.common.is_list_like=pd.api.types.is_list_like\n",
    "import pandas_datareader as pdr\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "vader=SentimentIntensityAnalyzer()\n",
    "import pickle\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl (636kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\kylej\\anaconda3\\lib\\site-packages (from textblob) (3.4)\n",
      "Requirement already satisfied: six in c:\\users\\kylej\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\users\\kylej\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (3.4.0.3)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.1.1, however version 19.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading https://files.pythonhosted.org/packages/86/9e/c53e1fc61aac5ee490a6ac5e21b1ac04e55a7c2aba647bb8411c9aadf24e/vaderSentiment-3.2.1-py2.py3-none-any.whl (125kB)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.1.1, however version 19.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Sentiment for each Tweet\n",
    "Here I define a function that takes in the dataframe of raw tweets, calculates the following things:\n",
    "- sentiment for each tweet using both TextBlob and Vader\n",
    "- weighted sentiment for both TextBlob and Vader (sentiment x followers)\n",
    "\n",
    "and adds them to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment_scores(df):\n",
    "    vader_score=[]\n",
    "    weighted_vader=[]\n",
    "    blob_score=[]\n",
    "    weighted_blob=[]\n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            text=df['Tweet content'][i]\n",
    "            follow=df.Followers[i]\n",
    "            vad=vader.polarity_scores(text)\n",
    "            blb=TextBlob(text)\n",
    "            vs=vad['compound']\n",
    "            bl=blb.sentiment[0]\n",
    "            vader_score.append(vs)\n",
    "            weighted_vader.append(vs*follow)\n",
    "            blob_score.append(bl)\n",
    "            weighted_blob.append(bl*follow)\n",
    "        except:\n",
    "            vader_score.append(0)\n",
    "            weighted_vader.append(0)\n",
    "            blob_score.append(0)\n",
    "            weighted_blob.append(0)\n",
    "    vader_score=pd.DataFrame(vader_score,columns=['vader_score'])\n",
    "    weighted_vader=pd.DataFrame(weighted_vader,columns=['weighted_vader'])\n",
    "    blob_score=pd.DataFrame(blob_score,columns=['blob_score'])\n",
    "    weighted_blob=pd.DataFrame(weighted_blob,columns=['weighted_blob'])\n",
    "    df=df.merge(vader_score,left_index=True,right_index=True)\n",
    "    df=df.merge(weighted_vader,left_index=True,right_index=True)\n",
    "    df=df.merge(blob_score,left_index=True,right_index=True)\n",
    "    df=df.merge(weighted_blob,left_index=True,right_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had trouble finding a way to iterate through this process and still be able to keep the namings the way that I wanted so I ended up using the cell below over and over by changing the ticker symbol for each stock that had a .csv file of tweets.  This is not an elegant process but I decided to go with it in order to keep the project moving. At the end of the cycle I ended up with a dataframe for each stock that contained the necessary sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adbe=pd.read_csv('raw tweet data/adbe.csv')\n",
    "adbe=add_sentiment_scores(adbe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next ugly part of the process was to add a column in each dataframe with the name of the stock so that it could be sorted by ticker later in the analysis phase.  After that I was able to combine them all into a single dataframe that contains all tweets for all stocks and then pickle it out for use in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aal['ticker']='aal'\n",
    "aapl['ticker']='aapl'\n",
    "adbe['ticker']='adbe'\n",
    "adp['ticker']='adp'\n",
    "adsk['ticker']='adsk'\n",
    "akam['ticker']='akam'\n",
    "alxn['ticker']='alxn'\n",
    "amat['ticker']='amat'\n",
    "amgn['ticker']='amgn'\n",
    "amzn['ticker']='amzn'\n",
    "atvi['ticker']='atvi'\n",
    "bbby['ticker']='bbby'\n",
    "bidu['ticker']='bidu'\n",
    "bmrn['ticker']='bmrn'\n",
    "ca['ticker']='ca'\n",
    "celg['ticker']='celg'\n",
    "cern['ticker']='cern'\n",
    "chkp['ticker']='chkp'\n",
    "chtr['ticker']='chtr'\n",
    "cmcsa['ticker']='cmcsa'\n",
    "cost['ticker']='cost'\n",
    "csco['ticker']='csco'\n",
    "csx['ticker']='csx'\n",
    "ctrp['ticker']='ctrp'\n",
    "dish['ticker']='dish'\n",
    "dltr['ticker']='dltr'\n",
    "ea['ticker']='ea'\n",
    "ebay['ticker']='ebay'\n",
    "endp['ticker']='endp'\n",
    "esrx['ticker']='esrx'\n",
    "expe['ticker']='expe'\n",
    "fast['ticker']='fast'\n",
    "fb['ticker']='fb'\n",
    "fisv['ticker']='fisv'\n",
    "gild['ticker']='gild'\n",
    "goog['ticker']='goog'\n",
    "hsic['ticker']='hsic'\n",
    "ilmn['ticker']='ilmn'\n",
    "incy['ticker']='incy'\n",
    "intu['ticker']='intu'\n",
    "isrg['ticker']='isrg'\n",
    "jd['ticker']='jd'\n",
    "khc['ticker']='khc'\n",
    "lbty['ticker']='lbty'\n",
    "lltc['ticker']='lltc'\n",
    "lmca['ticker']='lmca'\n",
    "lrcx['ticker']='lrcx'\n",
    "mar['ticker']='mar'\n",
    "mat['ticker']='mat'\n",
    "mdlz['ticker']='mdlz'\n",
    "mnst['ticker']='mnst'\n",
    "msft['ticker']='msft'\n",
    "mu['ticker']='mu'\n",
    "mxim['ticker']='mxim'\n",
    "myl['ticker']='myl'\n",
    "nclh['ticker']='nclh'\n",
    "nflx['ticker']='nflx'\n",
    "ntap['ticker']='ntap'\n",
    "nvda['ticker']='nvda'\n",
    "nxpi['ticker']='nxpi'\n",
    "orly['ticker']='orly'\n",
    "payx['ticker']='payx'\n",
    "pcar['ticker']='pcar'\n",
    "pcln['ticker']='pcln'\n",
    "pypl['ticker']='pypl'\n",
    "qcom['ticker']='qcom'\n",
    "regn['ticker']='regn'\n",
    "rost['ticker']='rost'\n",
    "sbac['ticker']='sbac'\n",
    "sbux['ticker']='sbux'\n",
    "sndk['ticker']='sndk'\n",
    "srcl['ticker']='srcl'\n",
    "stx['ticker']='stx'\n",
    "swks['ticker']='swks'\n",
    "symc['ticker']='symc'\n",
    "tmus['ticker']='tmus'\n",
    "trip['ticker']='trip'\n",
    "tsco['ticker']='tsco'\n",
    "tsla['ticker']='tsla'\n",
    "txn['ticker']='txn'\n",
    "ulta['ticker']='ulta'\n",
    "vod['ticker']='vod'\n",
    "vrsk['ticker']='vrsk'\n",
    "vrtx['ticker']='vrtx'\n",
    "wba['ticker']='wba'\n",
    "wdc['ticker']='wdc'\n",
    "wfm['ticker']='wfm'\n",
    "xlnx['ticker']='xlnx'\n",
    "yhoo['ticker']='yhoo'\n",
    "\n",
    "all_tweets=pd.concat([aal,aapl,adbe,adp,adsk,akam,alxn,amat,amgn,amzn,atvi,bbby,bidu,bmrn,ca,celg,cern,chkp,chtr,\n",
    "          cmcsa,cost,csco,csx,ctrp,dish,dltr,ea,ebay,endp,esrx,expe,fast,fb,fisv,gild,goog,hsic,\n",
    "          ilmn,incy,intu,isrg,jd,khc,lbty,lltc,lmca,lrcx,mar,mat,mdlz,mnst,msft,mu,mxim,myl,nclh,nflx,\n",
    "          ntap,nvda,nxpi,orly,payx,pcar,pcln,pypl,qcom,regn,rost,sbac,sbux,sndk,srcl,stx,swks,symc,tmus,\n",
    "          trip,tsco,tsla,txn,ulta,vod,vrsk,vrtx,wba,wdc,wfm,xlnx,yhoo],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next painstaking part of this process was to group each stock by industry and more broadly by sector.  Normally this would not be a difficult process but because of the limited number of stocks in the dataset, I had to lump some of them togther that may not have matched exactly.  I did this manually by referencing the company profile on yahoo.com/finance and then ran the cell below for each stock in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting for adding sector and industry\n",
    "tick='stx'\n",
    "sect='communication'\n",
    "ind='data_storage'\n",
    "all_tweets.loc[all_tweets['ticker'].str.contains(tick),'sector']=sect\n",
    "all_tweets.loc[all_tweets['ticker'].str.contains(tick),'industry']=ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that was done, I finally had the complete dataframe that I wanted and was able to pickle it out for use in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out=open(\"all_tweets.pickle\",\"wb\")\n",
    "pickle.dump(all_tweets,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Stock Movement Data\n",
    "Once I had all of the tweets in a useful format, I needed to gather the one day performance of each stock for each day in question.  This cell retrieves the data from the Yahoo Finance API for the relevant days and calcultes the Open to Close movement for each stock for each day and then pickles it out for later use. It is important to note that the stock movement does not include what may have happened when the market was closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathers daily return for each stock on each day of a tweet\n",
    "all_stocks=all_tweets.ticker.unique()\n",
    "all_dates=all_tweets.Date.unique()\n",
    "daily_returns=pd.DataFrame(all_dates,columns=['Date'])\n",
    "count=0\n",
    "for i in all_stocks:\n",
    "    print(i)\n",
    "    # this is to correct for stocks whose tickers have changed since 2016\n",
    "    if i=='lbty':\n",
    "        i='lbtya'\n",
    "    else:\n",
    "        None\n",
    "    if i=='pcln':\n",
    "        i='bkng'\n",
    "    else:\n",
    "        None\n",
    "    holder = pdr.get_data_yahoo(i,'2016-03-01','2016-06-30')\n",
    "    # '2016-03-01','2016-06-30'\n",
    "    holder['Date'] = holder.index\n",
    "    holder['Date'] = pd.to_datetime(holder['Date'], infer_datetime_format=True)\n",
    "    holder = holder.reset_index(drop=True)\n",
    "    holder['move']=(holder.Close-holder.Open)/holder.Open\n",
    "    holder.drop(['High','Low','Open','Close','Volume','Adj Close'],axis=1,inplace=True)\n",
    "    holder.rename(columns={'move':str(i)},inplace=True)\n",
    "    daily_returns=daily_returns.merge(holder,left_on='Date',right_on='Date')\n",
    "    count+=1\n",
    "pickle_out=open(\"daily_returns.pickle\",\"wb\")\n",
    "pickle.dump(daily_returnss,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final part of preparing the data is to create a dataframe that has each day's aggregate tweet scores and the subsequent price movement for each stock.  Below I also create tweet scores and group movements for the industry and sector that each stock is in so that we can compare these later on. Finally this dataframe is pickled out for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_returns=daily_returns.set_index('Date')\n",
    "all_stocks=all_tweets.ticker.unique()\n",
    "df=pd.DataFrame([])\n",
    "for i in all_stocks:\n",
    "    # isolating tweets about stock in question\n",
    "    stock_tweets=all_tweets[all_tweets.ticker==i]\n",
    "    stock_tweets.reset_index(inplace=True)\n",
    "    # get other stocks in sector\n",
    "    sect=stock_tweets.sector[0]\n",
    "    ind=stock_tweets.industry[0]\n",
    "    tweets_in_sector=all_tweets[all_tweets.sector==sect]\n",
    "    tweets_in_ind=all_tweets[all_tweets.industry==ind]\n",
    "    stocks_in_sector=tweets_in_sector.ticker.unique()\n",
    "    stocks_in_ind=tweets_in_ind.ticker.unique()\n",
    "    if i=='pcln':\n",
    "        i='bkng'\n",
    "    else:\n",
    "        None\n",
    "    for j in daily_returns.index:\n",
    "        try:\n",
    "            stock_move=daily_returns[i][j]\n",
    "            sect_moves=[]\n",
    "            for k in stocks_in_sector:\n",
    "                sect_moves.append(daily_returns[k][j])\n",
    "            ind_moves=[]\n",
    "            for l in stocks_in_ind:\n",
    "                ind_moves.append(daily_returns[l][j])\n",
    "            mean_sector_move=np.mean(sect_moves)\n",
    "            mean_ind_move=np.mean(ind_moves)\n",
    "            end_time=j+np.timedelta64(9,\"h\")+np.timedelta64(30,'m')\n",
    "            start_time=j-np.timedelta64(1,\"D\")+np.timedelta64(16,'h')\n",
    "            stock_tweets_day=stock_tweets[(stock_tweets.timestamp>start_time)&(stock_tweets.timestamp<end_time)]\n",
    "            sector_tweets_day=tweets_in_sector[(tweets_in_sector.timestamp>start_time)&(tweets_in_sector.timestamp<end_time)]\n",
    "            ind_tweets_day=tweets_in_ind[(tweets_in_ind.timestamp>start_time)&(tweets_in_ind.timestamp<end_time)]\n",
    "            df=df.append(pd.DataFrame({'stock':i,'trade_date':j,'sector':sect,'industry':ind,\n",
    "                                      'stock_vader':stock_tweets_day.vader_score.mean(),\n",
    "                                      'stock_weighted_vader':stock_tweets_day.weighted_vader.sum()/stock_tweets_day.Followers.sum(),\n",
    "                                      'stock_blob':stock_tweets_day.blob_score.mean(),\n",
    "                                      'stock_weighted_blob':stock_tweets_day.weighted_blob.sum()/stock_tweets_day.Followers.sum(),\n",
    "                                      'sector_vader':sector_tweets_day.vader_score.mean(),\n",
    "                                      'sector_weighted_vader':sector_tweets_day.weighted_vader.sum()/sector_tweets_day.Followers.sum(),\n",
    "                                      'sector_blob':sector_tweets_day.blob_score.mean(),\n",
    "                                      'sector_weighted_blob':sector_tweets_day.weighted_blob.sum()/sector_tweets_day.Followers.sum(),\n",
    "                                      'ind_vader':ind_tweets_day.vader_score.mean(),\n",
    "                                      'ind_weighted_vader':ind_tweets_day.weighted_vader.sum()/ind_tweets_day.Followers.sum(),\n",
    "                                      'ind_blob':ind_tweets_day.blob_score.mean(),\n",
    "                                      'ind_weighted_blob':ind_tweets_day.weighted_blob.sum()/ind_tweets_day.Followers.sum(),\n",
    "                                       'num_stock_tweets':len(stock_tweets_day),'num_sector_tweets':len(sector_tweets_day),\n",
    "                                       'num_ind_tweets':len(ind_tweets_day),'stock_tweets_reach':stock_tweets_day.Followers.sum(),\n",
    "                                       'sector_tweets_reach':sector_tweets_day.Followers.sum(),\n",
    "                                       'ind_tweets_reach':ind_tweets_day.Followers.sum(),\n",
    "                                      'stock_move':stock_move,'sector_move':mean_sector_move,'ind_move':mean_ind_move},index=[0]),\n",
    "                        ignore_index=True)\n",
    "        except:\n",
    "            None\n",
    "df=df.dropna()\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df.hist(figsize=(15,15),bins=20)\n",
    "df['stock_num_tweet_mean']=None\n",
    "df['stock_num_tweet_std']=None\n",
    "df['stock_tweet_sentiment_mean']=None\n",
    "df['stock_tweet_sentiment_std']=None\n",
    "sto=df.stock.unique()\n",
    "for i in sto:\n",
    "    temp=df[df.stock==i]\n",
    "    num_mean=temp.num_stock_tweets.mean()\n",
    "    num_std=temp.num_stock_tweets.std()\n",
    "    sent_mean=temp.stock_weighted_vader.mean()\n",
    "    sent_std=temp.stock_weighted_vader.std()\n",
    "    df.loc[df['stock'].str.contains(i),'stock_num_tweet_mean']=num_mean\n",
    "    df.loc[df['stock'].str.contains(i),'stock_num_tweet_std']=num_std\n",
    "    df.loc[df['stock'].str.contains(i),'stock_tweet_sentiment_mean']=sent_mean\n",
    "    df.loc[df['stock'].str.contains(i),'stock_tweet_sentiment_std']=sent_std\n",
    "pickle_out=open(\"df.pickle\",\"wb\")\n",
    "pickle.dump(df,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please continue to the notebook titled \"Notebook_2_Archived_Tweet_Analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
